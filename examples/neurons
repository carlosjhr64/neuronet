#!/usr/bin/env ruby
# frozen_string_literal: true

require 'neuronet'

# rubocop:disable Lint/RedundantCopDisableDirective
# rubocop:disable Lint/BinaryOperatorWithIdenticalOperands
def random
  rand - rand
end
# rubocop:enable Lint/BinaryOperatorWithIdenticalOperands
# rubocop:enable Lint/RedundantCopDisableDirective

# create the input nodes
a = Neuronet::Neuron.new
b = Neuronet::Neuron.new

# create the output neuron
sum = Neuronet::Neuron.new

# and a neuron on the side
adjuster = Neuronet::Neuron.new

# connect the adjuster to a and b
adjuster.connect(a)
adjuster.connect(b)

# connect sum to a and b
sum.connect(a)
sum.connect(b)
# and to the adjuster
sum.connect(adjuster)

Neuronet.noise = Neuronet::NO_NOISE

# Train the tiny network
100_000.times do
  a.value = x = random
  b.value = y = random
  sum.update
  target = x + y
  output = sum.value
  sum.backpropagate(0.25 * (target - output))
end

# Let's see how well the training went
e2 = 0.0
trials = 1000
trials.times do |trial|
  a.value = x = random
  b.value = y = random
  sum.update
  target = x + y
  output = sum.value
  error = target - output
  e2 += error * error
  next unless (trial % 100).zero?
  puts "#{x.round(3)} + #{y.round(3)} = #{target.round(3)}"
  puts "   Neuron says: #{output.round(3)}"
  puts "   Error: #{(100.0 * error / target).round(2)}%, #{error.round(3)}"
end
standard_deviation = Math.sqrt(e2 / (trials + 1))

puts "Mju: #{Neuronet::Neuron.mju(sum).round(2)}"
puts "Standard Deviation: #{standard_deviation.round(3)}"
