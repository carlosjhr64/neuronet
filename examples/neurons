#!/usr/bin/env ruby
# frozen_string_literal: true

require 'neuronet'

def random
  # rubocop:disable Lint/BinaryOperatorWithIdenticalOperands
  rand - rand
  # rubocop:enable Lint/BinaryOperatorWithIdenticalOperands
end

# create the input nodes
a = Neuronet::Neuron.new
b = Neuronet::Neuron.new

# create the output neuron
sum = Neuronet::Neuron.new

# and a neuron on the side
adjuster = Neuronet::Neuron.new

# connect the adjuster to a and b
adjuster.connect(a)
adjuster.connect(b)

# connect sum to a and b
sum.connect(a)
sum.connect(b)
# and to the adjuster
sum.connect(adjuster)

# TODO: Semantics of this constant vs mu and current Neuronet.learning
# The learning constant is about...
learning = 0.3

# Train the tiny network
1_000_000.times do
  a.value = x = random
  b.value = y = random
  sum.update
  target = x + y
  output = sum.value
  sum.backpropagate(learning * (target - output))
end

# Let's see how well the training went
e2 = 0.0
trials = 100
12.times do
  a.value = x = random
  b.value = y = random
  sum.update
  target = x + y
  output = sum.value
  error = target - output
  e2 += error * error
  puts "#{x.round(3)} + #{y.round(3)} = #{target.round(3)}"
  puts "  Neuron says #{output.round(3)}, " \
       "#{(100.0 * error / target).round(2)}% error."
end
standard_deviation = Math.sqrt(e2 / (trials + 1))

puts "Learning: #{learning}"
puts "Mu: #{sum.mu}"
puts "Standard Deviation: #{standard_deviation}"
