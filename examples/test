#!/usr/bin/env ruby
# frozen_string_literal: true

require 'neuronet'

### Squash/UnSquash ###
raise 'Expected version 7' unless /^7\./.match? Neuronet::VERSION
raise 'Bad Squash 0.0' unless Neuronet.squash[0.0] == 0.5
raise 'Bad Squash 1.0' unless Neuronet.squash[1.0].round(3) == 0.731
raise 'Bad Squash -1.0' unless Neuronet.squash[-1.0].round(3) == 0.269
raise 'Bad UnSquash 0.5' unless Neuronet.unsquash[0.5] == 0.0
raise 'Bad UnSquash 0.731' unless Neuronet.unsquash[0.731].round(3) == 1.0
raise 'Bad UnSquash 0.269' unless Neuronet.unsquash[0.269].round(3) == -1.0

### Noise ###
sum = 0.0
N = 10_000
N.times do
  sum += Neuronet.noise[1.0]
end
# Every once in a while by chance, this will happen.
unless (sum/N.to_f).round(1) == 1.0
  raise 'Noise not averaging to one? (Happens sometimes)'
end
sum = 0.0
N.times do
  sum += (1.0 - Neuronet.noise[1.0])**2.0
end
std = Math.sqrt(sum/N.to_f)
unless std.round(1) == 0.4
  raise "Standard Deviation #{std.round(2)} in noise not 0.4?"
end

### Node ###
node = Neuronet::Neuron.new
raise 'Bad node value' unless node.value == 0.0
raise 'Bad node activation' unless node.activation == 0.5
raise 'Bad node updated' unless node.update == 0.5
raise 'Bad node backpropagate' unless node.backpropagate(0.1).activation == 0.5
node = Neuronet::Neuron.new(1.0)
raise 'Bad node value' unless node.value.round(3) == 1.0
raise 'Bad node activation' unless node.activation.round(3) == 0.731
raise 'Bad node updated' unless node.update.round(3) == 0.731

### Connection ###
conn = Neuronet::Connection.new(node)
raise 'Bad connection weight' unless conn.weight == 0.0
raise 'Bad connection node' unless conn.neuron == node
raise 'Bad connection value' unless conn.weighted_activation == 0.0
# and a quick sanity check
raise 'Bad connection node' unless conn.neuron.activation == node.activation
conn = Neuronet::Connection.new(node, 1.0)
raise 'Bad connection weight' unless conn.weight == 1.0
raise 'Bad connection value' unless conn.activation == node.activation # * 1.0
# stays the same, of course:
raise 'Bad connection update' unless conn.update == node.activation
# Positive error increments weight
w = conn.weight
conn.backpropagate(0.1)
raise 'Positive error did not increment weight' unless conn.weight > w
w = conn.weight
conn.backpropagate(-0.1)
raise 'Negative error did not decrease weight' unless conn.weight < w

### Neuron ###
neuron = Neuronet::Neuron.new
# Just checking methods exist, really...
raise 'Connections to neuron now?' unless neuron.connections.empty?
raise 'Bad neuron bias'  unless neuron.bias.zero?
# IDK how to unit test the following methods... without a lot of work.
# Will just test by behavior later.
raise 'Missing partial?' unless neuron.respond_to?(:partial)
raise 'Missing backpropagate?' unless neuron.respond_to?(:backpropagate)
raise 'Missing connect?' unless neuron.respond_to?(:connect)

### InputLayer ###
input = Neuronet::Layer.new(5)
raise 'Bad Layer length' unless input.length == 5
raise 'Bad Layer node' unless input[0].instance_of? Neuronet::Neuron

### Layer ###
layer = Neuronet::Layer.new(6)
raise 'Bad Layer length' unless layer.length == 6
raise 'Bad layer neuron' unless layer[0].instance_of? Neuronet::Neuron
layer.connect(input)
unless layer.last.connections.last.neuron.connections.empty?
  raise "Where's the input node?"
end
# Will test the partial and train by behavior later
raise 'Missing partial?' unless layer.respond_to?(:partial)
raise 'Missing train?' unless layer.respond_to?(:train)
raise 'Bad layer values?' unless layer.values == [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

### FeedForward ###
ffn = Neuronet::FeedForward.new([4,3,2])
# TODO: Still working on mu and learning
# raise 'Unexpected mu' unless ffn.mu == 9.0
raise 'Unexpected learning' unless Neuronet.learning == 2.0
Neuronet.learning = 3.0
raise 'Could not set learning' unless Neuronet.learning == 3.0
Neuronet.learning = 2.0
raise 'Could not reset learning' unless Neuronet.learning == 2.0
unless ffn.all?{ _1.instance_of? Neuronet::Layer }
  raise 'ffn is not an Array of Layers'
end
raise 'entrada is not Layer?' unless ffn.entrada.instance_of? Neuronet::Layer
raise 'salida is not Layer?' unless ffn.salida.instance_of? Neuronet::Layer
raise 'yin is not Layer?' unless ffn.yin.instance_of? Neuronet::Layer
raise 'yant is not Layer?' unless ffn.yang.instance_of? Neuronet::Layer

# the rest of FeedForwardNetwork to be tested by behavior.
%i[update set train input output].each do |symbol|
  raise "ffn does not respond to #{symbol}." unless ffn.respond_to?(symbol)
end

### Scale ###
scale = Neuronet::Scale.new
raise 'bad scale center' unless scale.center.nil?
raise 'bad scale spread' unless scale.spread.nil?
scale.set([0,1,2,3,4,5,6,7,8,9,10])
raise 'Unexpected spread' unless scale.spread == 5
raise 'Unexpected center' unless scale.center == 5
mapped = scale.mapped([0,1,2,3,4,5,6,7,8,9,10])
raise 'mapped first should be -1' unless mapped.first == -1.0
raise 'mapped last should be 1' unless mapped.last == 1.0
unless scale.unmapped(mapped).map{|x| x.round(3)} == [0,1,2,3,4,5,6,7,8,9,10]
  raise 'Could not unmap'
end

### Gaussian ###
scale = Neuronet::Gaussian.new
raise 'bad scale center' unless scale.center.nil?
raise 'bad scale spread' unless scale.spread.nil?
scale.set([0,1,2,3,4,5,6,7,8,9,10])
raise 'Unexpected spread' unless scale.spread.round(1) == 3.3
raise 'Unexpected center' unless scale.center == 5
mapped = scale.mapped([0,1,2,3,4,5,6,7,8,9,10])
raise 'mapped first should be -1' unless mapped.first.round(1) == -1.5
raise 'mapped last should be 1' unless mapped.last.round(1) == 1.5
unless scale.unmapped(mapped).map{|x| x.round(3)} == [0,1,2,3,4,5,6,7,8,9,10]
  raise 'Could not unmap'
end

### LogNormal ###
scale = Neuronet::LogNormal.new
raise 'bad scale center' unless scale.center.nil?
raise 'bad scale spread' unless scale.spread.nil?
scale.set([1.0,2.0,4.0,8.0,16.0,32.0,64.0,256.0])
raise 'Unexpected spread' unless scale.spread.round(2) == 1.85
raise 'Unexpected center' unless scale.center.round(2) == 2.51
mapped = scale.mapped([1.0,2.0,4.0,8.0,16.0,32.0,64.0,256.0])
raise 'mapped first should be -1' unless mapped.first.round(2) == -1.36
raise 'mapped last should be 1' unless mapped.last.round(2) == 1.64
raise 'Could not unmap' unless scale.unmapped(mapped).map{|x| x.round(3)} ==
                               [1.0,2.0,4.0,8.0,16.0,32.0,64.0,256.0]

# I'm going to gloss over the rest because
# they mostly depend on stuff already tested above.

### ScaledNetwork ###
ffn = Neuronet::ScaledNetwork.new([8,5,3])
unless ffn.distribution.instance_of? Neuronet::Gaussian
  raise 'ScaledNetwork should have Guassian.'
end
raise 'ScaledNetwork should respond to reset' unless ffn.respond_to?(:reset)

### Tao ###
ffn = Neuronet::Tao.bless Neuronet::ScaledNetwork.new([6,5,4,3,2])
# TODO: Still working on mu and learning
# raise 'Unexpected mu' unless ffn.mu == 40.0
raise 'Yin should be a layer.' unless ffn.yin.instance_of? Neuronet::Layer
raise 'Yin should be the first middle layer' unless ffn.yin.length == 5
raise 'Yang should be a layer.' unless ffn.yang.instance_of? Neuronet::Layer
raise 'Yang should be the last middle layer.' unless ffn.yang.length == 3
# a quick sanity check
raise 'The last layer should be the output layer' unless ffn.salida == ffn.last
a = ffn.last.last.connections.last.neuron
b = ffn.first.last
unless a.instance_of?(Neuronet::Neuron) && a == b
  raise 'The output layer should be connected to the input layer.'
end

### Yin ###
ffn = Neuronet::Yin.bless Neuronet::FeedForward.new([3,3,3])
0.upto(ffn.yin.length-1) do |i|
  unless ffn.yin[i].connections[i].weight == Neuronet::WONE
    raise 'Yin is supposed to initially mirror input'
  end
  raise 'Yin should have BZERO bias.' unless ffn.yin[i].bias == Neuronet::BZERO
end

### Yang ###
complained = false
begin
  ffn = Neuronet::Yang.bless Neuronet::FeedForward.new([3,3,5])
rescue
  complained = true
end
raise 'Should have complained about Output longer than Yang' unless complained
ffn = Neuronet::Yang.bless Neuronet::FeedForward.new([3,3,3])
0.upto(ffn.salida.length-1) do |i|
  unless ffn.salida[i].connections[i].weight == Neuronet::WONE
    raise 'Output is supposed to initially mirror Yang'
  end
  unless ffn.salida[i].bias == Neuronet::BZERO
    raise 'Output with Yang should have BZERO bias.'
  end
end

### TaoYinYang ###
ffn = Neuronet::TaoYinYang[3]
# TODO: Still working on mu and learning
# raise 'Unexpected mu' unless ffn.mu == 13.5
